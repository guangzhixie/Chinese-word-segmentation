{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinese Word Segmentation using RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_file = 'data/training/msr_training.utf8'\n",
    "test_file = 'data/testing/msr_test.utf8'\n",
    "result_file = 'data/result/msr_rnn.utf8'\n",
    "cws_info_filePath = 'rnn/cws.info'\n",
    "cws_data_filePath = 'rnn/cws.data'\n",
    "output_model_file = 'rnn/msr_training_word2vec.model'\n",
    "output_vector_file = 'rnn/msr_training_word2vec.vector'\n",
    "output_keras_model_file = 'rnn/cws_keras_model'\n",
    "output_keras_model_weights_file = 'rnn/keras_model_weights'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import h5py\n",
    "import string\n",
    "import codecs\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\"\"\"\n",
    "vocab: char -> index\n",
    "indexVocab: list of char ordered by the index in vocab\n",
    "initProb: probability for each tag\n",
    "tranProb: Transition probability for transition from tag 1 to tag 2\n",
    "X: list of [char_1, char_2, .. char_ctxWindows]\n",
    "y: list of tag indices\n",
    "\"\"\"\n",
    "\n",
    "corpus_tags = ['S', 'B', 'M', 'E']\n",
    "retain_unknown = 'retain-unknown'\n",
    "retain_padding = 'retain-padding'\n",
    "\n",
    "def saveCwsInfo(path, cwsInfo):\n",
    "    print('save cws info to %s'%path)\n",
    "    fd = open(path, 'w')\n",
    "    (initProb, tranProb), (vocab, indexVocab) = cwsInfo\n",
    "    j = json.dumps((initProb, tranProb))\n",
    "    fd.write(j + '\\n')\n",
    "    for char in vocab:\n",
    "        fd.write(char.encode('utf-8') + '\\t' + str(vocab[char]) + '\\n')\n",
    "    fd.close()\n",
    "\n",
    "def loadCwsInfo(path):\n",
    "    print('load cws info from %s'%path)\n",
    "    fd = open(path, 'r')\n",
    "    line = fd.readline()\n",
    "    j = json.loads(line.strip())\n",
    "    initProb, tranProb = j[0], j[1]\n",
    "    lines = fd.readlines()\n",
    "    fd.close()\n",
    "    vocab = {}\n",
    "    indexVocab = [0 for i in range(len(lines))]\n",
    "    for line in lines:\n",
    "        rst = line.strip().split('\\t')\n",
    "        if len(rst) < 2: continue\n",
    "        char, index = rst[0].decode('utf-8'), int(rst[1])\n",
    "        vocab[char] = index\n",
    "        indexVocab[index] = char\n",
    "    return (initProb, tranProb), (vocab, indexVocab)\n",
    "\n",
    "def saveCwsData(path, cwsData):\n",
    "    '''Save training samples'''\n",
    "    print('save cws data to %s' % path)\n",
    "    #use hdf5 with high efficiency\n",
    "    fd = h5py.File(path,'w')\n",
    "    (X, y) = cwsData\n",
    "    fd.create_dataset('X', data = X)\n",
    "    fd.create_dataset('y', data = y)\n",
    "    fd.close()\n",
    "\n",
    "def loadCwsData(path):\n",
    "    '''load training samples'''\n",
    "    print('load cws data from %s' % path)\n",
    "    fd = h5py.File(path,'r')\n",
    "    X = fd['X'][:]\n",
    "    y = fd['y'][:]\n",
    "    fd.close()\n",
    "    return (X, y)\n",
    "\n",
    "def sent2vec2(sent, vocab, ctxWindows = 5):\n",
    "\n",
    "    charVec = []\n",
    "    for char in sent:\n",
    "        if char in vocab:\n",
    "            charVec.append(vocab[char])\n",
    "        else:\n",
    "            charVec.append(vocab[retain_unknown])\n",
    "    # padding on head and tail\n",
    "    num = len(charVec)\n",
    "    pad = int((ctxWindows - 1)/2)\n",
    "    for i in range(pad):\n",
    "        charVec.insert(0, vocab[retain_padding] ) # sentence head\n",
    "        charVec.append(vocab[retain_padding] ) # sentence tail\n",
    "    X = []\n",
    "    for i in range(num):\n",
    "        X.append(charVec[i:i + ctxWindows])\n",
    "    return X\n",
    "\n",
    "def sent2vec(sent, vocab, ctxWindows = 5):\n",
    "    chars = []\n",
    "    for char in sent:\n",
    "        chars.append(char)\n",
    "    return sent2vec2(chars, vocab, ctxWindows = ctxWindows)\n",
    "\n",
    "def doc2vec(fname, vocab):\n",
    "    fd = codecs.open(fname, 'r', 'utf-8')\n",
    "    lines = fd.readlines()\n",
    "    fd.close()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    tagSize = len(corpus_tags)\n",
    "    tagCnt = [0 for i in range(tagSize)]\n",
    "    tagTranCnt = [[0 for i in range(tagSize)] for j in range(tagSize)]\n",
    "    \n",
    "    sentCnt = 0\n",
    "    initTagCnt = [0 for i in range(tagSize)]\n",
    "\n",
    "    for line in lines:\n",
    "        words = line.strip('\\n').split()\n",
    "        chars = []\n",
    "        tags = []\n",
    "        for word in words:\n",
    "            if len(word) > 1:\n",
    "                chars.append(word[0])\n",
    "                tags.append(corpus_tags.index('B'))\n",
    "                for char in word[1:(len(word) - 1)]:\n",
    "                    chars.append(char)\n",
    "                    tags.append(corpus_tags.index('M'))\n",
    "                chars.append(word[-1])\n",
    "                tags.append(corpus_tags.index('E'))\n",
    "            else: \n",
    "                chars.append(word)\n",
    "                tags.append(corpus_tags.index('S'))\n",
    "                \n",
    "        sentCnt += 1\n",
    "        if len(words) > 0:\n",
    "            if len(words[0]) > 1:\n",
    "                initTagCnt[corpus_tags.index('B')] += 1\n",
    "            else:\n",
    "                initTagCnt[corpus_tags.index('S')] += 1\n",
    "\n",
    "        lineVecX = sent2vec2(chars, vocab, ctxWindows = 7)\n",
    "\n",
    "        lineVecY = []\n",
    "        lastTag = -1\n",
    "        for tag in tags:\n",
    "            lineVecY.append(tag)\n",
    "            tagCnt[tag] += 1\n",
    "            if lastTag != -1:\n",
    "                tagTranCnt[lastTag][tag] += 1\n",
    "            lastTag = tag\n",
    "\n",
    "        X.extend(lineVecX)\n",
    "        y.extend(lineVecY)\n",
    "\n",
    "    charCnt = sum(tagCnt)\n",
    "    tranCnt = sum([sum(tag) for tag in tagTranCnt])\n",
    "    initProb = []\n",
    "    for i in range(tagSize):\n",
    "        initProb.append(initTagCnt[i]/float(sentCnt))\n",
    "    tranProb = []\n",
    "    for i in range(tagSize):\n",
    "        p = []\n",
    "        for j in range(tagSize):\n",
    "            p.append(tagTranCnt[i][j]/float(tranCnt))\n",
    "        tranProb.append(p)\n",
    "\n",
    "    return X, y, initProb, tranProb\n",
    "\n",
    "def genVocab(fname, delimiters = [' ', '\\n']):\n",
    "    fd = codecs.open(fname, 'r', 'utf-8')\n",
    "    data = fd.read()\n",
    "    fd.close()\n",
    "\n",
    "    vocab = {}\n",
    "    indexVocab = []\n",
    "    index = 0\n",
    "    for char in data:\n",
    "        if char not in delimiters and char not in vocab:\n",
    "            vocab[char] = index\n",
    "            indexVocab.append(char)\n",
    "            index += 1\n",
    "\n",
    "    vocab[retain_unknown] = len(vocab)\n",
    "    vocab[retain_padding] = len(vocab)\n",
    "    indexVocab.append(retain_unknown)\n",
    "    indexVocab.append(retain_padding)\n",
    "    return vocab, indexVocab\n",
    "\n",
    "def load(fname):\n",
    "    print 'train from file', fname\n",
    "    vocab, indexVocab = genVocab(fname)\n",
    "    X, y, initProb, tranProb = doc2vec(fname, vocab)\n",
    "    print 'Total characters: ', len(X), len(y)\n",
    "    print 'Total vocab: ', len(vocab), len(indexVocab)\n",
    "    print 'Init prob: ', initProb\n",
    "    print 'Transition prob: ', tranProb\n",
    "    return (X, y), (initProb, tranProb), (vocab, indexVocab)\n",
    "\n",
    "\n",
    "\n",
    "def preProcess():\n",
    "    start_time = time.time()\n",
    "    (X, y), (initProb, tranProb), (vocab, indexVocab) = load(input_file)\n",
    "    saveCwsInfo(cws_info_filePath, ((initProb, tranProb), (vocab, indexVocab)))\n",
    "    saveCwsData(cws_data_filePath, (X, y))\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"used time : %d s\" % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train from file data/training/msr_training.utf8\n",
      "Total characters:  4050469 4050469\n",
      "Total vocab:  5171 5171\n",
      "Init prob:  [0.30438083843357416, 0.6955501357507707, 0.0, 0.0]\n",
      "Transition prob:  [[0.10778314698107833, 0.1514084213877909, 0.0, 0.0], [0.0, 0.0, 0.05428188006159124, 0.26236725602874794], [0.0, 0.0, 0.05345560079837499, 0.05428188006159124], [0.16643509822378974, 0.14998671645703562, 0.0, 0.0]]\n",
      "save cws info to rnn/cws.info\n",
      "save cws data to rnn/cws.data\n",
      "used time : 18 s\n"
     ]
    }
   ],
   "source": [
    "preProcess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# import modules & set up logging\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import multiprocessing\n",
    "import time\n",
    "import json\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "def output_vocab(vocab):\n",
    "    for k, v in vocab.items():\n",
    "        print(k)\n",
    "        \n",
    "def run_word2vec():\n",
    "    start_time = time.time()\n",
    "     \n",
    "    model = Word2Vec(LineSentence(input_file), size=128, window=5, min_count=5, workers=multiprocessing.cpu_count())\n",
    " \n",
    "    model.init_sims(replace=True)\n",
    "    model.save(output_model_file)\n",
    "    model.wv.save_word2vec_format(output_vector_file, binary=False)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"used time : %d s\" % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used time : 19 s\n"
     ]
    }
   ],
   "source": [
    "run_word2vec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import h5py\n",
    "import codecs\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers.core import Dense, Dropout, Activation, TimeDistributedDense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(cwsInfo, cwsData, modelPath, weightPath):\n",
    "\n",
    "    (initProb, tranProb), (vocab, indexVocab) = cwsInfo\n",
    "    (X, y) = cwsData\n",
    "\n",
    "    train_X, test_X, train_y, test_y = model_selection.train_test_split(X, y , train_size=0.9, random_state=1)\n",
    "\n",
    "    train_X = np.array(train_X)\n",
    "    train_y = np.array(train_y)\n",
    "    test_X = np.array(test_X)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    outputDims = len(corpus_tags)\n",
    "    Y_train = np_utils.to_categorical(train_y, outputDims)\n",
    "    Y_test = np_utils.to_categorical(test_y, outputDims)\n",
    "    batchSize = 128\n",
    "    vocabSize = len(vocab) + 1\n",
    "    wordDims = 100\n",
    "    maxlen = 7\n",
    "    hiddenDims = 100\n",
    "\n",
    "    w2vModel = Word2Vec.load(output_model_file)\n",
    "    embeddingDim = w2vModel.vector_size\n",
    "    embeddingUnknown = [0 for i in range(embeddingDim)]\n",
    "    embeddingWeights = np.zeros((vocabSize + 1, embeddingDim))\n",
    "    for word, index in vocab.items():\n",
    "        if word in w2vModel:\n",
    "            e = w2vModel[word]\n",
    "        else:\n",
    "            e = embeddingUnknown\n",
    "        embeddingWeights[index, :] = e\n",
    "\n",
    "    #LSTM\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(output_dim = embeddingDim, input_dim = vocabSize + 1, \n",
    "        input_length = maxlen, mask_zero = True, weights = [embeddingWeights]))\n",
    "    model.add(LSTM(output_dim = hiddenDims, return_sequences = True))\n",
    "    model.add(LSTM(output_dim = hiddenDims, go_backwards = True, return_sequences = False))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(outputDims))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics=[\"accuracy\"])\n",
    "\n",
    "    result = model.fit(train_X, Y_train, batch_size = batchSize, \n",
    "                    nb_epoch = 5, validation_data = (test_X,Y_test))\n",
    "\n",
    "    j = model.to_json()\n",
    "    fd = open(modelPath, 'w')\n",
    "    fd.write(j)\n",
    "    fd.close()\n",
    "\n",
    "    model.save_weights(weightPath)\n",
    "\n",
    "    return model\n",
    "\n",
    "def start_train():\n",
    "    print 'Loading vocab...'\n",
    "    start_time = time.time()\n",
    "    cwsInfo = loadCwsInfo(cws_info_filePath)\n",
    "    cwsData = loadCwsData(cws_data_filePath)\n",
    "    print(\"Loading used time : \", time.time() - start_time)\n",
    "    print 'Done!'\n",
    "\n",
    "    print 'Training model...'\n",
    "    start_time = time.time()\n",
    "    model = train(cwsInfo, cwsData, output_keras_model_file, output_keras_model_weights_file)\n",
    "    print(\"Training used time : \", time.time() - start_time)\n",
    "    print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vocab...\n",
      "load cws info from rnn/cws.info\n",
      "load cws data from rnn/cws.data\n",
      "('Loading used time : ', 0.17533612251281738)\n",
      "Done!\n",
      "Training model...\n",
      "Train on 3645422 samples, validate on 405047 samples\n",
      "Epoch 1/5\n",
      "3645422/3645422 [==============================] - 1586s - loss: 0.2665 - acc: 0.9034 - val_loss: 0.1592 - val_acc: 0.9437\n",
      "Epoch 2/5\n",
      "3645422/3645422 [==============================] - 1535s - loss: 0.1325 - acc: 0.9546 - val_loss: 0.1199 - val_acc: 0.9588\n",
      "Epoch 3/5\n",
      "3645422/3645422 [==============================] - 1533s - loss: 0.0994 - acc: 0.9661 - val_loss: 0.1023 - val_acc: 0.9648\n",
      "Epoch 4/5\n",
      "3645422/3645422 [==============================] - 1579s - loss: 0.0819 - acc: 0.9722 - val_loss: 0.0904 - val_acc: 0.9694\n",
      "Epoch 5/5\n",
      "3645422/3645422 [==============================] - 1497s - loss: 0.0713 - acc: 0.9758 - val_loss: 0.0836 - val_acc: 0.9716\n",
      "('Training used time : ', 7736.648426055908)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "start_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string_util\n",
    "StringUtil = string_util.StringUtil()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi(obs, states, start_p, trans_p, emit_p):\n",
    "\n",
    "    lenObs = len(obs)\n",
    "    lenStates = len(states)\n",
    "\n",
    "    V = [[0.0 for col in range(lenStates)] for row in range(lenObs)]\n",
    "    path = [[0 for col in range(lenObs)] for row in range(lenStates)]\n",
    "\n",
    "    #t = 0\n",
    "    for y in range(lenStates):\n",
    "        #V[0][y] = start_p[y] * emit_p[y][obs[0]]\n",
    "        V[0][y] = start_p[y] * emit_p[y][0]\n",
    "        path[y][0] = y\n",
    "\n",
    "    #t > 1\n",
    "    for t in range(1, lenObs):\n",
    "        newpath = [[0.0 for col in range(lenObs)] for row in range(lenStates)]\n",
    "\n",
    "        for y in range(lenStates):\n",
    "            prob = -1\n",
    "            state = 0\n",
    "            for y0 in range(lenStates):\n",
    "                #nprob = V[t - 1][y0] * trans_p[y0][y] * emit_p[y][obs[t]]\n",
    "                nprob = V[t - 1][y0] * trans_p[y0][y] * emit_p[y][t]\n",
    "                if nprob > prob:\n",
    "                    prob = nprob\n",
    "                    state = y0\n",
    "                    # max prob\n",
    "                    V[t][y] = prob\n",
    "                    # back pointer\n",
    "                    newpath[y][:t] = path[state][:t]\n",
    "                    newpath[y][t] = y\n",
    "\n",
    "        path = newpath\n",
    "\n",
    "    prob = -1\n",
    "    state = 0\n",
    "    for y in range(lenStates):\n",
    "        if V[lenObs - 1][y] > prob:\n",
    "            prob = V[lenObs - 1][y]\n",
    "            state = y\n",
    "\n",
    "    return prob, path[state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### -*- coding: utf-8 -*-\n",
    "\n",
    "def loadModel(modelPath, weightPath):\n",
    "\n",
    "    fd = open(modelPath, 'r')\n",
    "    j = fd.read()\n",
    "    fd.close()\n",
    "\n",
    "    model = model_from_json(j)\n",
    "\n",
    "    model.load_weights(weightPath)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def cwsSent(sent, model, cwsInfo):\n",
    "    (initProb, tranProb), (vocab, indexVocab) = cwsInfo\n",
    "    vec = sent2vec(sent, vocab, ctxWindows = 7)\n",
    "    vec = np.array(vec)\n",
    "    probs = model.predict_proba(vec, verbose=0)\n",
    "    #classes = model.predict_classes(vec)\n",
    "\n",
    "    prob, path = viterbi(vec, corpus_tags, initProb, tranProb, probs.transpose())\n",
    "\n",
    "    output = ''\n",
    "    total_len = len(path)\n",
    "    for i, t in enumerate(path):\n",
    "        output += sent[i]\n",
    "        if i < total_len - 1:\n",
    "            if corpus_tags[t] == 'S' or corpus_tags[t] == 'E':\n",
    "                output += StringUtil.SPACE\n",
    "\n",
    "    return output\n",
    "\n",
    "def cwsFile(fname, dstname, model, cwsInfo):\n",
    "    fd = codecs.open(fname, 'r', 'utf-8')\n",
    "    lines = fd.readlines()\n",
    "    fd.close()\n",
    "\n",
    "    fd = open(dstname, 'w')\n",
    "    for line in lines:\n",
    "        rst = cwsSent(line.strip(), model, cwsInfo)\n",
    "        fd.write(rst.encode('utf-8') + '\\n')\n",
    "    fd.close()\n",
    "\n",
    "def test():\n",
    "    cwsInfo = loadCwsInfo(cws_info_filePath)\n",
    "    print('Loading model...')\n",
    "    start_time = time.time()\n",
    "    model = loadModel(output_keras_model_file, output_keras_model_weights_file)\n",
    "    print(\"Loading used time : \", time.time() - start_time)\n",
    "    print('Done!')\n",
    "    \n",
    "    print 'Doing segmentation for the test file', test_file, '...'\n",
    "    cwsFile(test_file, result_file, model, cwsInfo)\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test predict a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cws info from rnn/cws.info\n",
      "星期天 我们 去 吃 重庆 火锅\n"
     ]
    }
   ],
   "source": [
    "s = u'星期天我们去吃重庆火锅'\n",
    "cwsInfo = loadCwsInfo(cws_info_filePath)\n",
    "model = loadModel(output_keras_model_file, output_keras_model_weights_file)\n",
    "print cwsSent(s, model, cwsInfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do segmentation for the test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cws info from rnn/cws.info\n",
      "Loading model...\n",
      "('Loading used time : ', 0.6088700294494629)\n",
      "Done!\n",
      "Doing segmentation for the test file data/testing/msr_test.utf8 ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result:\n",
      "\n",
      "扬帆 远东 做 与 中国 合作 的 先行\r\n",
      "希腊 的 经济 结构 较 特殊 。\r\n",
      "海 运业 雄踞 全球 之 首 ， 按 吨位 计 占 世界 总数 的 １７％ 。\r\n",
      "另外 旅游 、 侨汇 也是 经济 收入 的 重要 组成部分 ， 制造业 规模 相对 较小 。\r\n",
      "多年来 ， 中 希 贸易 始终 处于 较低 的 水平 ， 希腊 几乎 没有 在 中国 投资 。\r\n",
      "十几年 来 ， 改革开放 的 中国 经济 高速 发展 ， 远东 在 崛起 。\r\n",
      "瓦西里斯 的 船只 中 有 ４０％ 驶 向 远东 ， 每个 月 几乎 都 有 两三条 船 停靠 中国 港口 。\r\n",
      "他 感受 到 了 中国 经济 发展 的 大潮 。\r\n",
      "他 要 与 中国人 合作 。\r\n",
      "他 来到 中国 ， 成为 第一个 访 华 的 大 船主 。\r\n"
     ]
    }
   ],
   "source": [
    "print 'Test result:\\n'\n",
    "!head data/result/msr_rnn.utf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold:\n",
      "\n",
      "扬帆  远东  做  与  中国  合作  的  先行  \r",
      "\r\n",
      "希腊  的  经济  结构  较  特殊  。\r",
      "\r\n",
      "海运  业  雄踞  全球  之  首  ，  按  吨位  计  占  世界  总数  的  １７％  。\r",
      "\r\n",
      "另外  旅游  、  侨汇  也是  经济  收入  的  重要  组成部分  ，  制造业  规模  相对  较小  。\r",
      "\r\n",
      "多年来  ，  中  希  贸易  始终  处于  较低  的  水平  ，  希腊  几乎  没有  在  中国  投资  。\r",
      "\r\n",
      "十几年  来  ，  改革开放  的  中国  经济  高速  发展  ，  远东  在  崛起  。\r",
      "\r\n",
      "瓦西里斯  的  船只  中  有  ４０％  驶  向  远东  ，  每个  月  几乎  都  有  两三条  船  停靠  中国  港口  。\r",
      "\r\n",
      "他  感受  到  了  中国  经济  发展  的  大潮  。\r",
      "\r\n",
      "他  要  与  中国人  合作  。\r",
      "\r\n",
      "他  来到  中国  ，  成为  第一个  访  华  的  大  船主  。\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "print 'Gold:\\n'\n",
    "!head data/gold/msr_test_gold.utf8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate accuracy using MSR gold set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import izip\n",
    "\n",
    "def tag_for_sentence(sentence):\n",
    "    words = sentence.decode('utf-8').strip().split()\n",
    "    tags = []\n",
    "    for word in words:\n",
    "        if len(word) > 1:\n",
    "            tags.append('b')\n",
    "            for char in word[1:(len(word) - 1)]:\n",
    "                tags.append('m')\n",
    "            tags.append('e')\n",
    "        else: \n",
    "            tags.append('s')\n",
    "            \n",
    "    return tags\n",
    "\n",
    "\n",
    "def tag_for_file(input_path, output_path):\n",
    "    print 'Tagging for %s and output to %s...' % (input_path, output_path)\n",
    "    start = time.time()\n",
    "    \n",
    "    with open(output_path, \"w+\") as output_file:\n",
    "        for line in open(input_path, \"r\").readlines():\n",
    "            tags = tag_for_sentence(line)\n",
    "            for tag in tags:\n",
    "                output_file.writelines(tag + StringUtil.NEWLINE)\n",
    "    \n",
    "    print 'Done. Total time taken %d seconds' % (time.time() - start)\n",
    "    \n",
    "    \n",
    "def print_model_accuracy(result_file_path, gold_standard_file_path):\n",
    "    count = 0\n",
    "    correct = 0\n",
    "    with open(gold_standard_file_path, 'r') as expect_file, open(result_file_path, 'r') as result_file:\n",
    "        for expect_line, result_line in izip(expect_file, result_file):\n",
    "            count += 1\n",
    "            if expect_line.strip() == result_line.strip():\n",
    "                correct +=1\n",
    "\n",
    "    print \"Accuracy: {0}\".format(float(correct)/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging for data/result/msr_rnn.utf8 and output to data/result/msr_rnn_tag.utf8...\n",
      "Done. Total time taken 0 seconds\n"
     ]
    }
   ],
   "source": [
    "tag_for_file(\"data/result/msr_rnn.utf8\", \"data/result/msr_rnn_tag.utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging for data/gold/msr_test_gold.utf8 and output to data/gold/tag_msr.utf8...\n",
      "Done. Total time taken 0 seconds\n"
     ]
    }
   ],
   "source": [
    "tag_for_file(\"data/gold/msr_test_gold.utf8\", \"data/gold/tag_msr.utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.956865829514\n"
     ]
    }
   ],
   "source": [
    "print_model_accuracy(\"data/result/msr_rnn_tag.utf8\", \"data/gold/tag_msr.utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.visualize_util import plot\n",
    "\n",
    "model = loadModel(output_keras_model_file, output_keras_model_weights_file)\n",
    "plot(model, to_file='img/rnn_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN model diagram](img/rnn_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test MSR trained model against PKU test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_file = 'data/testing/pku_test.utf8'\n",
    "result_file = 'data/result/pku_rnn.utf8'\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging for data/result/pku_rnn.utf8 and output to data/result/pku_rnn_tag.utf8...\n",
      "Done. Total time taken 0 seconds\n"
     ]
    }
   ],
   "source": [
    "tag_for_file(\"data/result/pku_rnn.utf8\", \"data/result/pku_rnn_tag.utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging for data/gold/pku_test_gold.utf8 and output to data/gold/tag_pku.utf8...\n",
      "Done. Total time taken 0 seconds\n"
     ]
    }
   ],
   "source": [
    "tag_for_file(\"data/gold/pku_test_gold.utf8\", \"data/gold/tag_pku.utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.842629954901\n"
     ]
    }
   ],
   "source": [
    "print_model_accuracy(\"data/result/pku_rnn_tag.utf8\", \"data/gold/tag_pku.utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model with PKU corpus and test against PKU test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_file = 'data/training/pku_training.utf8'\n",
    "test_file = 'data/testing/pku_test.utf8'\n",
    "result_file = 'data/result/pku_rnn.utf8'\n",
    "cws_info_filePath = 'rnn/cws_pku.info'\n",
    "cws_data_filePath = 'rnn/cws_pku.data'\n",
    "output_model_file = 'rnn/pku_training_word2vec.model'\n",
    "output_vector_file = 'rnn/pku_training_word2vec.vector'\n",
    "output_keras_model_file = 'rnn/pku_cws_keras_model'\n",
    "output_keras_model_weights_file = 'rnn/pku_keras_model_weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train from file data/training/pku_training.utf8\n",
      "Total characters:  1826448 1826448\n",
      "Total vocab:  4701 4701\n",
      "Init prob:  [0.28729041286694174, 0.3204175536341577, 0.07187447986474294, 0.3204175536341577]\n",
      "Transition prob:  [[0.12090999527496495, 0.16095605053463716, 0.0, 0.0], [0.0, 0.0, 0.047525332052668096, 0.2762701436432787], [0.0, 0.0, 0.025106866571428254, 0.047525332052668096], [0.16555659695672334, 0.15614968291363146, 0.0, 0.0]]\n",
      "save cws info to rnn/cws_pku.info\n",
      "save cws data to rnn/cws_pku.data\n",
      "used time : 7 s\n"
     ]
    }
   ],
   "source": [
    "preProcess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used time : 8 s\n"
     ]
    }
   ],
   "source": [
    "run_word2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vocab...\n",
      "load cws info from rnn/cws_pku.info\n",
      "load cws data from rnn/cws_pku.data\n",
      "('Loading used time : ', 0.09951400756835938)\n",
      "Done!\n",
      "Training model...\n",
      "Train on 1643803 samples, validate on 182645 samples\n",
      "Epoch 1/5\n",
      "1643803/1643803 [==============================] - 737s - loss: 0.2768 - acc: 0.9002 - val_loss: 0.1558 - val_acc: 0.9474\n",
      "Epoch 2/5\n",
      "1643803/1643803 [==============================] - 716s - loss: 0.1266 - acc: 0.9584 - val_loss: 0.1092 - val_acc: 0.9635\n",
      "Epoch 3/5\n",
      "1643803/1643803 [==============================] - 712s - loss: 0.0874 - acc: 0.9715 - val_loss: 0.0881 - val_acc: 0.9710\n",
      "Epoch 4/5\n",
      "1643803/1643803 [==============================] - 723s - loss: 0.0655 - acc: 0.9788 - val_loss: 0.0754 - val_acc: 0.9759\n",
      "Epoch 5/5\n",
      "1643803/1643803 [==============================] - 750s - loss: 0.0517 - acc: 0.9831 - val_loss: 0.0685 - val_acc: 0.9780\n",
      "('Training used time : ', 3642.9727289676666)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "start_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging for data/result/pku_rnn.utf8 and output to data/result/pku_rnn_tag.utf8...\n",
      "Done. Total time taken 0 seconds\n"
     ]
    }
   ],
   "source": [
    "tag_for_file(\"data/result/pku_rnn.utf8\", \"data/result/pku_rnn_tag.utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging for data/gold/pku_test_gold.utf8 and output to data/gold/tag_pku.utf8...\n",
      "Done. Total time taken 0 seconds\n"
     ]
    }
   ],
   "source": [
    "tag_for_file(\"data/gold/pku_test_gold.utf8\", \"data/gold/tag_pku.utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9107350651\n"
     ]
    }
   ],
   "source": [
    "print_model_accuracy(\"data/result/pku_rnn_tag.utf8\", \"data/gold/tag_pku.utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test PKU trained model against MSR test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_file = 'data/testing/msr_test.utf8'\n",
    "result_file = 'data/result/msr_rnn.utf8'\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging for data/result/msr_rnn.utf8 and output to data/result/msr_rnn_tag.utf8...\n",
      "Done. Total time taken 0 seconds\n"
     ]
    }
   ],
   "source": [
    "tag_for_file(\"data/result/msr_rnn.utf8\", \"data/result/msr_rnn_tag.utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging for data/gold/msr_test_gold.utf8 and output to data/gold/tag_msr.utf8...\n",
      "Done. Total time taken 0 seconds\n"
     ]
    }
   ],
   "source": [
    "tag_for_file(\"data/gold/msr_test_gold.utf8\", \"data/gold/tag_msr.utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.869051558135\n"
     ]
    }
   ],
   "source": [
    "print_model_accuracy(\"data/result/msr_rnn_tag.utf8\", \"data/gold/tag_msr.utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model with combined corpus and test again combined test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_file = 'data/training/data.utf8'\n",
    "test_file = 'data/testing/combined/test.utf8'\n",
    "result_file = 'data/result/combined_rnn.utf8'\n",
    "cws_info_filePath = 'rnn/cws_combined.info'\n",
    "cws_data_filePath = 'rnn/cws_combined.data'\n",
    "output_model_file = 'rnn/combined_training_word2vec.model'\n",
    "output_vector_file = 'rnn/combined_training_word2vec.vector'\n",
    "output_keras_model_file = 'rnn/combined_cws_keras_model'\n",
    "output_keras_model_weights_file = 'rnn/combined_keras_model_weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train from file data/training/data.utf8\n",
      "Total characters:  5880019 5880019\n",
      "Total vocab:  5416 5416\n",
      "Init prob:  [0.2786111745557285, 0.31317602885296797, 0.09503676773833554, 0.31317602885296797]\n",
      "Transition prob:  [[0.1118399027631214, 0.1543448400321375, 0.0, 0.0], [0.0, 0.0, 0.05218757863996186, 0.2667420744340824], [0.0, 0.0, 0.04459519070250285, 0.05218757863996186], [0.16610093521911637, 0.15200189956911578, 0.0, 0.0]]\n",
      "save cws info to rnn/cws_combined.info\n",
      "save cws data to rnn/cws_combined.data\n",
      "used time : 24 s\n"
     ]
    }
   ],
   "source": [
    "preProcess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used time : 27 s\n"
     ]
    }
   ],
   "source": [
    "run_word2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vocab...\n",
      "load cws info from rnn/cws_combined.info\n",
      "load cws data from rnn/cws_combined.data\n",
      "('Loading used time : ', 0.22170186042785645)\n",
      "Done!\n",
      "Training model...\n",
      "Train on 5292017 samples, validate on 588002 samples\n",
      "Epoch 1/5\n",
      "5292017/5292017 [==============================] - 2327s - loss: 0.2798 - acc: 0.8930 - val_loss: 0.1933 - val_acc: 0.9248\n",
      "Epoch 2/5\n",
      "5292017/5292017 [==============================] - 2389s - loss: 0.1750 - acc: 0.9323 - val_loss: 0.1631 - val_acc: 0.9361\n",
      "Epoch 3/5\n",
      "3683328/5292017 [===================>..........] - ETA: 679s - loss: 0.1501 - acc: 0.9413"
     ]
    }
   ],
   "source": [
    "start_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging for data/result/combined_rnn.utf8 and output to data/result/combined_rnn_tag.utf8...\n",
      "Done. Total time taken 0 seconds\n"
     ]
    }
   ],
   "source": [
    "tag_for_file(\"data/result/combined_rnn.utf8\", \"data/result/combined_rnn_tag.utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging for data/gold/combined/test.utf8 and output to data/gold/combined/tag_combined.utf8...\n",
      "Done. Total time taken 0 seconds\n"
     ]
    }
   ],
   "source": [
    "tag_for_file(\"data/gold/combined/test.utf8\", \"data/gold/combined/tag_combined.utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.905878498156\n"
     ]
    }
   ],
   "source": [
    "print_model_accuracy(\"data/result/combined_rnn_tag.utf8\", \"data/gold/combined/tag_combined.utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cws info from rnn/cws_combined.info\n"
     ]
    }
   ],
   "source": [
    "cwsInfo = loadCwsInfo(cws_info_filePath)\n",
    "model = loadModel(output_keras_model_file, output_keras_model_weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "上海 自来水 来自 海上\n"
     ]
    }
   ],
   "source": [
    "s = u'上海自来水来自海上'\n",
    "print cwsSent(s, model, cwsInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "星期天 我们 去 吃 重庆 火锅\n"
     ]
    }
   ],
   "source": [
    "s = u'星期天我们去吃重庆火锅'\n",
    "print cwsSent(s, model, cwsInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "权力 的 游戏 开播 了\n"
     ]
    }
   ],
   "source": [
    "s = u'权力的游戏开播了'\n",
    "print cwsSent(s, model, cwsInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
